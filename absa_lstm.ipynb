{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHZT2maSuvyZ"
   },
   "source": [
    "Loading the datasets to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1715257244004,
     "user": {
      "displayName": "Dinesh Kumar",
      "userId": "05478840007381211685"
     },
     "user_tz": -480
    },
    "id": "m4KzmDwHrvsc",
    "outputId": "c94101e5-8f3d-40e4-8c99-a1e2e6f33470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence         aspect  polarity\n",
      "0  It might be the best sit down food I've had in...           food  positive\n",
      "1  It might be the best sit down food I've had in...          place   neutral\n",
      "2  Hostess was extremely accommodating when we ar...          staff  positive\n",
      "3  Hostess was extremely accommodating when we ar...  miscellaneous   neutral\n",
      "4  We were a couple of minutes late for our reser...  miscellaneous   neutral\n",
      "                                            sentence         aspect  polarity\n",
      "0  I would wait for a table next time, the food w...  miscellaneous   neutral\n",
      "1  I would wait for a table next time, the food w...           food  positive\n",
      "2  We did complain to the manager, but she just s...          staff  negative\n",
      "3  We did complain to the manager, but she just s...           food   neutral\n",
      "4  the service was inattentive (didn't bring us w...        service  negative\n",
      "                                            sentence   aspect  polarity\n",
      "0  We went again and sat at the bar this time, I ...    place   neutral\n",
      "1  We went again and sat at the bar this time, I ...     food  negative\n",
      "2  The food was good, but it's not worth the wait...     food  positive\n",
      "3  The food was good, but it's not worth the wait...  service  negative\n",
      "4  Waiter took our drink order and then we didn't...    staff  negative\n"
     ]
    }
   ],
   "source": [
    "#Load the datasets\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON data into a Python dictionary\n",
    "with open('train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract data from the dictionary\n",
    "columns, index, data_values = data[\"columns\"], data[\"index\"], data[\"data\"]\n",
    "\n",
    "# Create a DataFrame using the extracted data\n",
    "train_df = pd.DataFrame(data_values, columns=columns, index=index)\n",
    "\n",
    "# Load JSON data into a Python dictionary for val.json\n",
    "with open('val.json', 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Extract data from the val.json dictionary\n",
    "val_columns, val_index, val_data_values = val_data[\"columns\"], val_data[\"index\"], val_data[\"data\"]\n",
    "\n",
    "# Create a DataFrame for val.json using the extracted data\n",
    "val_df = pd.DataFrame(val_data_values, columns=val_columns, index=val_index)\n",
    "\n",
    "# Load JSON data into a Python dictionary for test.json\n",
    "with open('test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Extract data from the test.json dictionary\n",
    "test_columns, test_index, test_data_values = test_data[\"columns\"], test_data[\"index\"], test_data[\"data\"]\n",
    "\n",
    "# Create a DataFrame for test.json using the extracted data\n",
    "test_df = pd.DataFrame(test_data_values, columns=test_columns, index=test_index)\n",
    "\n",
    "#check whether the data is loaded successfully\n",
    "print(train_df.head())\n",
    "print(val_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8qo0JrFu3Jl"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 160812,
     "status": "ok",
     "timestamp": 1715258314505,
     "user": {
      "displayName": "Dinesh Kumar",
      "userId": "05478840007381211685"
     },
     "user_tz": -480
    },
    "id": "PlYG2e3ttIwc"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import inflect\n",
    "\n",
    "# Function to remove punctuation using regular expressions\n",
    "def remove_punctuation_re(x):\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "    return x\n",
    "# Apply punctuation removal to the 'sentence' column in train_df, val_df, and test_df\n",
    "train_df['preprocessed_sentence'] = train_df['sentence'].apply(remove_punctuation_re)\n",
    "val_df['preprocessed_sentence'] = val_df['sentence'].apply(remove_punctuation_re)\n",
    "test_df['preprocessed_sentence'] = test_df['sentence'].apply(remove_punctuation_re)\n",
    "\n",
    "# Download NLTK resources\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for tokenization and stopwords removal\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    stop_words = set(sw.words())  # Get English stopwords\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]  # Remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize tokens\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)  # Join tokens back into a string\n",
    "    return preprocessed_text\n",
    "# Apply preprocessing to the 'preprocessed_sentence' column in train_df, val_df, and test_df\n",
    "train_df['preprocessed_sentence'] = train_df['preprocessed_sentence'].apply(preprocess_text)\n",
    "val_df['preprocessed_sentence'] = val_df['preprocessed_sentence'].apply(preprocess_text)\n",
    "test_df['preprocessed_sentence'] = test_df['preprocessed_sentence'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "# Function for text normalization\n",
    "def normalize_text(text):\n",
    "    # Normalize numbers: replace digits with their word forms\n",
    "    words = text.split()\n",
    "    normalized_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            normalized_word = p.number_to_words(word)\n",
    "            normalized_words.append(normalized_word)\n",
    "        else:\n",
    "            normalized_words.append(word)\n",
    "    normalized_text = ' '.join(normalized_words)\n",
    "    \n",
    "    # Normalize dates: replace dates with a placeholder token\n",
    "    normalized_text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', 'DATE', normalized_text)  # Example: 1/1/21, 10/31/2022, etc.\n",
    "    \n",
    "    # Add more normalization rules as needed\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# Apply text normalization to the 'preprocessed_sentence' column in train_df, val_df, and test_df\n",
    "train_df['preprocessed_sentence'] = train_df['preprocessed_sentence'].apply(normalize_text)\n",
    "val_df['preprocessed_sentence'] = val_df['preprocessed_sentence'].apply(normalize_text)\n",
    "test_df['preprocessed_sentence'] = test_df['preprocessed_sentence'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 160812,
     "status": "ok",
     "timestamp": 1715258314505,
     "user": {
      "displayName": "Dinesh Kumar",
      "userId": "05478840007381211685"
     },
     "user_tz": -480
    },
    "id": "PlYG2e3ttIwc"
   },
   "outputs": [],
   "source": [
    "# Tokenize and tag the text data using NLTK\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "    pos_tags = nltk.pos_tag(tokens)  # Perform POS tagging\n",
    "    return pos_tags\n",
    "\n",
    "# Apply POS tagging to the 'preprocessed_sentence' column in train_df, val_df, and test_df\n",
    "train_df['pos_tags'] = train_df['preprocessed_sentence'].apply(pos_tagging)\n",
    "val_df['pos_tags'] = val_df['preprocessed_sentence'].apply(pos_tagging)\n",
    "test_df['pos_tags'] = test_df['preprocessed_sentence'].apply(pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence', 'aspect', 'polarity', 'preprocessed_sentence', 'pos_tags'], dtype='object')\n",
      "0    food ive area upright citizen brigade garden p...\n",
      "1    food ive area upright citizen brigade garden p...\n",
      "2    hostess extremely accommodating arrived hour e...\n",
      "3    hostess extremely accommodating arrived hour e...\n",
      "4    couple minute late reservation minus guest des...\n",
      "Name: preprocessed_sentence, dtype: object\n",
      "0    [(food, NN), (ive, JJ), (area, NN), (upright, ...\n",
      "1    [(food, NN), (ive, JJ), (area, NN), (upright, ...\n",
      "2    [(hostess, NN), (extremely, RB), (accommodatin...\n",
      "3    [(hostess, NN), (extremely, RB), (accommodatin...\n",
      "4    [(couple, JJ), (minute, NN), (late, JJ), (rese...\n",
      "Name: pos_tags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "print(train_df.preprocessed_sentence.head())\n",
    "print(train_df.pos_tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence', 'aspect', 'polarity', 'preprocessed_sentence', 'pos_tags'], dtype='object')\n",
      "0                                 wait table time food\n",
      "1                                 wait table time food\n",
      "2          complain manager problem kitchen drink bill\n",
      "3          complain manager problem kitchen drink bill\n",
      "4    service inattentive bring wine course served o...\n",
      "Name: preprocessed_sentence, dtype: object\n",
      "0    [(wait, NN), (table, JJ), (time, NN), (food, NN)]\n",
      "1    [(wait, NN), (table, JJ), (time, NN), (food, NN)]\n",
      "2    [(complain, NN), (manager, NN), (problem, NN),...\n",
      "3    [(complain, NN), (manager, NN), (problem, NN),...\n",
      "4    [(service, NN), (inattentive, JJ), (bring, JJ)...\n",
      "Name: pos_tags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(val_df.columns)\n",
    "print(val_df.preprocessed_sentence.head())\n",
    "print(val_df.pos_tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence', 'aspect', 'polarity', 'preprocessed_sentence', 'pos_tags'], dtype='object')\n",
      "0    sat bar time five pint guinness buyback ordere...\n",
      "1    sat bar time five pint guinness buyback ordere...\n",
      "2                      food worth waitor lousy service\n",
      "3                      food worth waitor lousy service\n",
      "4                    waiter drink order fifteen minute\n",
      "Name: preprocessed_sentence, dtype: object\n",
      "0    [(sat, JJ), (bar, NN), (time, NN), (five, CD),...\n",
      "1    [(sat, JJ), (bar, NN), (time, NN), (five, CD),...\n",
      "2    [(food, NN), (worth, NN), (waitor, NN), (lousy...\n",
      "3    [(food, NN), (worth, NN), (waitor, NN), (lousy...\n",
      "4    [(waiter, NN), (drink, NN), (order, NN), (fift...\n",
      "Name: pos_tags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns)\n",
    "print(test_df.preprocessed_sentence.head())\n",
    "print(test_df.pos_tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define constants\n",
    "embedding_dim = 100  # Example embedding dimension\n",
    "aspect_size = 8      # Number of unique aspects\n",
    "output_size = 3      # Number of sentiment classes\n",
    "pos_embedding_dim = 50  # Embedding dimension for POS tags\n",
    "hidden_size = 64\n",
    "\n",
    "# Build vocabulary\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "word_idx = 0\n",
    "\n",
    "for dataframe in [train_df, val_df, test_df]:\n",
    "    for sentence in dataframe['preprocessed_sentence']:\n",
    "        for word in sentence.split():\n",
    "            if word not in word_to_idx:\n",
    "                word_to_idx[word] = word_idx\n",
    "                idx_to_word[word_idx] = word\n",
    "                word_idx += 1\n",
    "\n",
    "# Add padding token to vocabulary\n",
    "word_to_idx['<PAD>'] = word_idx\n",
    "idx_to_word[word_idx] = '<PAD>'\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Define a mapping from sentiment labels to indices\n",
    "label_to_idx = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "# Create a mapping from pos tags to indices\n",
    "all_pos_tags = set()\n",
    "for dataframe in [train_df, val_df, test_df]:\n",
    "    dataframe['pos_tags'].apply(lambda tags: all_pos_tags.update(tag for _, tag in tags))\n",
    "pos_to_idx = {tag: idx for idx, tag in enumerate(all_pos_tags)}\n",
    "pos_to_idx['<UNK>'] = len(pos_to_idx)  # Add unknown token for any tags not seen in training\n",
    "pos_vocab_size = len(pos_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, label_to_idx, aspect_to_idx, pos_to_idx):\n",
    "        self.dataframe = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.aspect_to_idx = aspect_to_idx\n",
    "        self.pos_to_idx = pos_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = self.dataframe['preprocessed_sentence'].iloc[idx]\n",
    "        aspect = self.dataframe['aspect'].iloc[idx]\n",
    "        label = self.dataframe['polarity'].iloc[idx]\n",
    "        pos_tags = self.dataframe['pos_tags'].iloc[idx]\n",
    "\n",
    "        # Convert input sentence to numerical format (example conversion to indices)\n",
    "        input_indices = [self.word_to_idx.get(word, self.word_to_idx['<PAD>']) for word in input_sentence.split()]\n",
    "        input_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "\n",
    "        # Convert aspect to numerical format (example conversion to index)\n",
    "        aspect_tensor = torch.tensor(self.aspect_to_idx[aspect], dtype=torch.long)\n",
    "\n",
    "        # Convert label to tensor (convert string label to index)\n",
    "        label_tensor = torch.tensor(self.label_to_idx[label], dtype=torch.long)\n",
    "\n",
    "        # Convert pos tags to numerical format\n",
    "        pos_indices = [self.pos_to_idx.get(pos, self.pos_to_idx['<UNK>']) for word, pos in pos_tags]\n",
    "        pos_tensor = torch.tensor(pos_indices, dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            'input': input_tensor,\n",
    "            'aspect': aspect_tensor,\n",
    "            'label': label_tensor,\n",
    "            'pos_tags': pos_tensor\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "        \n",
    "#define padding function\n",
    "def collate_fn(batch):\n",
    "    inputs = [item['input'] for item in batch]\n",
    "    aspects = [item['aspect'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    pos_tags = [item['pos_tags'] for item in batch]  # Add pos_tags to the collate function\n",
    "\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=word_to_idx['<PAD>'])\n",
    "    aspects_padded = torch.stack(aspects)\n",
    "    labels_padded = torch.stack(labels)\n",
    "    pos_tags_padded = torch.nn.utils.rnn.pad_sequence(pos_tags, batch_first=True, padding_value=pos_to_idx['<UNK>'])  # Add padding for pos_tags\n",
    "\n",
    "    return {'input': inputs_padded, 'aspect': aspects_padded, 'label': labels_padded, 'pos_tags': pos_tags_padded}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Variant 1: LSTM with Aspect Concatenation\n",
    "class LSTMWithAspectConcatenation(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, aspect_size):\n",
    "        super(LSTMWithAspectConcatenation, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.aspect_embedding = nn.Embedding(aspect_size, hidden_size)  # Aspect embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input_tensor, aspect_tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        aspect_embed = self.aspect_embedding(aspect_tensor)\n",
    "        combined = torch.cat((lstm_output[:, -1, :], aspect_embed), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n",
    "\n",
    "# Model Variant 2: LSTM with Aspect Attention\n",
    "class LSTMWithAspectAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, aspect_size):\n",
    "        super(LSTMWithAspectAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.aspect_embedding = nn.Embedding(aspect_size, hidden_size)  # Aspect embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_size * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_tensor, aspect_tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        aspect_embed = self.aspect_embedding(aspect_tensor).unsqueeze(1).repeat(1, lstm_output.size(1), 1)\n",
    "        combined = torch.cat((lstm_output, aspect_embed), dim=2)\n",
    "        attn_weights = torch.softmax(self.attn(combined), dim=1)\n",
    "        attn_output = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        output = self.fc(attn_output)\n",
    "        return output\n",
    "\n",
    "# Model Variant 3: LSTM with Aspect Fusion using POS Tagging\n",
    "class LSTMWithAspectFusionPOS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, pos_vocab_size, pos_embedding_dim, aspect_size):\n",
    "        super(LSTMWithAspectFusionPOS, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_embedding_dim)\n",
    "        self.aspect_embedding = nn.Embedding(aspect_size, hidden_size)  # Aspect embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_tensor, pos_tensor, aspect_tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        pos_embed = self.pos_embedding(pos_tensor)\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        aspect_embed = self.aspect_embedding(aspect_tensor).unsqueeze(1).repeat(1, lstm_output.size(1), 1)\n",
    "        fused_output = lstm_output + aspect_embed\n",
    "        output = self.fc(fused_output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTMWithAspectConcatenation(Model1) with hidden size 64\n",
      "Epoch 1/10, Train Loss: 0.9474, Val Loss: 0.9022\n",
      "Epoch 2/10, Train Loss: 0.9229, Val Loss: 0.9024\n",
      "Epoch 3/10, Train Loss: 0.9214, Val Loss: 0.9061\n",
      "Epoch 4/10, Train Loss: 0.9045, Val Loss: 0.8680\n",
      "Epoch 5/10, Train Loss: 0.8406, Val Loss: 0.8485\n",
      "Epoch 6/10, Train Loss: 0.7782, Val Loss: 0.8329\n",
      "Epoch 7/10, Train Loss: 0.7313, Val Loss: 0.8311\n",
      "Epoch 8/10, Train Loss: 0.6879, Val Loss: 0.8461\n",
      "Epoch 9/10, Train Loss: 0.6581, Val Loss: 0.9014\n",
      "Epoch 10/10, Train Loss: 0.6320, Val Loss: 0.8839\n",
      "\n",
      "Training LSTMWithAspectAttention(Model2) with hidden size 64\n",
      "Epoch 1/10, Train Loss: 1.0645, Val Loss: 1.0160\n",
      "Epoch 2/10, Train Loss: 1.0050, Val Loss: 0.9979\n",
      "Epoch 3/10, Train Loss: 0.9599, Val Loss: 0.9958\n",
      "Epoch 4/10, Train Loss: 0.9165, Val Loss: 0.9922\n",
      "Epoch 5/10, Train Loss: 0.8764, Val Loss: 1.0111\n",
      "Epoch 6/10, Train Loss: 0.8413, Val Loss: 1.0321\n",
      "Epoch 7/10, Train Loss: 0.8128, Val Loss: 1.0818\n",
      "Epoch 8/10, Train Loss: 0.7921, Val Loss: 1.0982\n",
      "Epoch 9/10, Train Loss: 0.7707, Val Loss: 1.1749\n",
      "Epoch 10/10, Train Loss: 0.7581, Val Loss: 1.2167\n",
      "\n",
      "Training LSTMWithAspectFusionPOS(Model3) with hidden size 64\n",
      "Epoch 1/10, Train Loss: 0.9550, Val Loss: 0.9116\n",
      "Epoch 2/10, Train Loss: 0.9240, Val Loss: 0.9073\n",
      "Epoch 3/10, Train Loss: 0.9216, Val Loss: 0.9075\n",
      "Epoch 4/10, Train Loss: 0.9154, Val Loss: 0.8958\n",
      "Epoch 5/10, Train Loss: 0.8731, Val Loss: 0.8493\n",
      "Epoch 6/10, Train Loss: 0.7996, Val Loss: 0.8141\n",
      "Epoch 7/10, Train Loss: 0.7442, Val Loss: 0.8363\n",
      "Epoch 8/10, Train Loss: 0.6991, Val Loss: 0.8392\n",
      "Epoch 9/10, Train Loss: 0.6647, Val Loss: 0.8760\n",
      "Epoch 10/10, Train Loss: 0.6379, Val Loss: 0.9247\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from aspect labels to indices\n",
    "aspect_to_idx = {aspect: idx for idx, aspect in enumerate(train_df['aspect'].unique())}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(train_df, word_to_idx, label_to_idx, aspect_to_idx, pos_to_idx)\n",
    "val_dataset = CustomDataset(val_df, word_to_idx, label_to_idx, aspect_to_idx, pos_to_idx)\n",
    "test_dataset = CustomDataset(test_df, word_to_idx, label_to_idx, aspect_to_idx, pos_to_idx)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, pos_tensor_required=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, aspects, labels = batch['input'], batch['aspect'], batch['label']\n",
    "            if pos_tensor_required:\n",
    "                pos_tensor = batch['pos_tags']  # Ensure `pos_tags` is included in the batch\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs, pos_tensor, aspects)\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs, aspects)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, aspects, labels = batch['input'], batch['aspect'], batch['label']\n",
    "                if pos_tensor_required:\n",
    "                    pos_tensor = batch['pos_tags']\n",
    "                    outputs = model(inputs, pos_tensor, aspects)\n",
    "                else:\n",
    "                    outputs = model(inputs, aspects)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Define your models\n",
    "model1 = LSTMWithAspectConcatenation(vocab_size, embedding_dim, hidden_size, output_size, len(aspect_to_idx))\n",
    "model2 = LSTMWithAspectAttention(vocab_size, embedding_dim, hidden_size, output_size, len(aspect_to_idx))\n",
    "model3 = LSTMWithAspectFusionPOS(vocab_size, embedding_dim, hidden_size, output_size, pos_vocab_size, pos_embedding_dim, len(aspect_to_idx))\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define your optimizer and specify model parameters and learning rate\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
    "\n",
    "# Train models\n",
    "print(f'\\nTraining LSTMWithAspectConcatenation(Model1) with hidden size {hidden_size}')\n",
    "train_model(model1, train_loader, val_loader, criterion, optimizer1, num_epochs=10)\n",
    "print(f'\\nTraining LSTMWithAspectAttention(Model2) with hidden size {hidden_size}')\n",
    "train_model(model2, train_loader, val_loader, criterion, optimizer2, num_epochs=10)\n",
    "print(f'\\nTraining LSTMWithAspectFusionPOS(Model3) with hidden size {hidden_size}')\n",
    "train_model(model3, train_loader, val_loader, criterion, optimizer3, num_epochs=10, pos_tensor_required=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGz4+ekJ2me3eIXHNeGwki",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
